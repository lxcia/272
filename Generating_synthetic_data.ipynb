{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMoPK3pk0DVAy66JdA4RCJu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Notebook authored by Bowen Jiang"],"metadata":{"id":"NDCfR8n5guUI"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"2iqupMQBqTsy","executionInfo":{"status":"ok","timestamp":1685212769274,"user_tz":420,"elapsed":2624,"user":{"displayName":"Bowen Jiang","userId":"16505810748545332074"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9c54931e-284b-4432-80de-8009f94cc512"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["#Basics \n","import numpy as np\n","import pandas as pd \n","import math, random\n","import sys, os \n","import sklearn\n","from scipy import stats \n","import matplotlib \n","import matplotlib.pyplot as plt \n","%matplotlib inline\n","\n","#Google Drive setup \n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["#Navigate to directory with Google Drive\n","%cd /content/drive/Shareddrives/CS\\ 272\\ PCOS\\ FL/Synthetic\\ Data\\ Generation\\ /"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h3Y3m6aQeciw","executionInfo":{"status":"ok","timestamp":1685212769274,"user_tz":420,"elapsed":19,"user":{"displayName":"Bowen Jiang","userId":"16505810748545332074"}},"outputId":"70ff8b74-ae9c-45c2-d239-e8286dfa33aa"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/Shareddrives/CS 272 PCOS FL/Synthetic Data Generation \n"]}]},{"cell_type":"code","source":["# Dictionary of hormone distributions\n","# Mean, SD\n","pcos_hormones = {\n","    \"lh_fsh_ratio\" : (1.7,0.7),\n","    \"estradiol\" : (87.6,21.1),\n","    \"testosterone\" : (71.4,27.9),\n","    \"progesterone_17oh\" : (1.6,1),\n","    \"dheas\" : (777.5,1135.8),\n","    \"androstenedione\" : (5.2,4.3),\n","    \"amh\" : (76.0, 36.3)\n","}\n","\n","normal_hormones = { ##Not used in this iteration of the code\n","    \"lh_fsh_ratio\" : (1.1,0.6),\n","    \"estradiol\" : (69.0,27.4),\n","    \"testosterone\" : (42.7,15.6),\n","    \"progesterone_17oh\" : (1.5,0.6),\n","    \"dheas\" : (1278.4,471.5),\n","    \"androstenedione\" : (1.4,0.5),    \n","}"],"metadata":{"id":"UHmYUL2xfdxS","executionInfo":{"status":"ok","timestamp":1685212769275,"user_tz":420,"elapsed":19,"user":{"displayName":"Bowen Jiang","userId":"16505810748545332074"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","************* KNOBS FOR ML LEARNING: *****************\n","  - QUANTILE_WIDTH: This sets the minimum quantile from which to sample response type means.\n","  For example, QUANTILE_WIDTH = 0 and bins=10 means that one response type will have a mean \n","  drawn from 0-10% lowest decile of the distribution; one mean drawn from 10-20% decile; etc.\n","  Setting quantile width higher (~0.4 for example) means that the maximal \"spread\" of hormone\n","  metric means will be narrower, making learning more challenging as the model will have to \n","  distinguish between subtle differences in the distribution of hormones.\n","\n","  -RESAMPLING_CONSTANT: This value is a measure of the \"narrowness\" of the standard deviations \n","  of hormone metrics. Setting this number higher means that the hormone metrics sampled for \n","  each patient response type will have less spread - coupled with a small value of QUANTILE_WIDTH,\n","  this can result in very separate distributions of hormone values per patient response type, making\n","  learning very easy. Note that RESAMPLING_CONSTANT should be at least 3 or so to prevent the \n","  generation of too many negative hormone values (due to the skewedness of those distributions \n","  and our use of the multivariate normal to generate data - it's exceedingly hard, i.e. statistics\n","  publication level hard, to generate data with a desired mean, covariance structure, and non-normal\n","  structure with a specified minimum value of 0). \n","\n","  -COVARIANCE_STRENGTH: A scaling factor for the off-diagonal elements in the covariance matrix. \n","  Small values of this constant result in minimal covariance in the generated data, which may \n","  make learning easier by maximizing the informativeness of each variable. Large values of this\n","  constant could make learning impossible. \n","\n","  -PROPORTION_TRUE_TREATMENT: The proportion of each patient response type that gets assigned the\n","  \"true\" OCP medication. What we attempt to model here is the fact that, even if patients have a\n","  feature profile that predisposes them to a certain most effective medication, their doctors may\n","  not prescribe it to them, or they are still in the process of trying out other medications, or\n","  there is actually a different medication that is most effective for them (due to some unmeasured\n","  covariates). Setting this proportion to 1 means that every patient of a given response type \n","  has the same (successful) medication prescribed to them. Setting this proportion to 0 means that\n","  within this response type, there are 10 groups of ~10% of the patients, each group being prescribed\n","  a different most effective medication. Ultimately, this proportion controls the maximum expected\n","  accuracy of our model: acc_max = PROPORTION_TRUE_TREATMENT + 0.1*(1-PROPORTION_TRUE_TREATMENT)\n","\n","  -PER_CLINIC_NOISE_SPREAD: A scaling factor for how much spread in the noise amounts across all\n","  of the different clinics. The idea is to generate some amount of normally-distributed, \n","  0-mean noise to each measurement, where the SD is some multiple of the mean measurement whose \n","  magnitude is controlled by the HORMONE_MEASUREMENT_NOISE constant. Can also control this distribution\n","  by re-tuning the gamma distribution parameters in the appropriate cell. \n","\n","  -HORMONE_MEASUREMENT_NOISE: A scaling factor for how much noise to add to each hormone measurement. \n","\n","  -SYMPTOM_MASK_CONST: A scaling factor for how much spread in the symptom-masking rate across all\n","  of the different clinics. Can also control this distribution by re-tuning the gamma distribution \n","  parameters in the appropriate cell. \n","\"\"\"\n","QUANTILE_WIDTH = 0.30 #Values in range [0, 0.5) -> lower number = easier learning\n","RESAMPLING_CONSTANT = 5 #Values in range [1, +inf] -> higher number = easier learning \n","COVARIANCE_STRENGTH = 1 #Values in range (0, +inf) but best to keep around/below 1 -> lower number = easier learning \n","PROPORTION_TRUE_TREATMENT = 0.8 #Values in range [0, 1] -> higher number = easier learning \n","PER_CLINIC_NOISE_SPREAD = 1 #Values in range [0, +inf) -> lower number = easier learning \n","HORMONE_MEASUREMENT_NOISE = 1 #Values in range [0, +inf) -> lower number = easier learning \n","SYMPTOM_MASK_CONST = 5 #"],"metadata":{"id":"2cClzYETGGzT","executionInfo":{"status":"ok","timestamp":1685212769275,"user_tz":420,"elapsed":18,"user":{"displayName":"Bowen Jiang","userId":"16505810748545332074"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Set random seed for downstream!\n","np.random.seed(30)\n","hormone_ranges = {}\n","\n","# Generate the ranges for each hormone\n","for hormone, dist in pcos_hormones.items():\n","    mean, sd = dist\n","    # Get the number of ranges to create\n","    hormone_num_ranges = np.random.randint(5,11)\n","    print(f\"Number of ranges for {hormone}: \"+str(hormone_num_ranges))\n","\n","    #Set boundaries for each patient response type mean\n","    \"\"\"\n","    Long comment - we really don't want negative hormone measurements as they\n","    don't make biological sense. However, there's really no easy way to parametrically \n","    generate correlated, multivariate datapoints that are non-normal (i.e. we \n","    can specify a minimum value of 0, a desired mean and a standard deviation \n","    for each variaable/feature along with inter-variable correlation). If we \n","    can do that, we might as well publish it. \n","\n","    Instead, we will generate multivariate normal data, but lower the SD so that\n","    negative datapoints are less likely. Our goal is to ensure that we don't \n","    select a response type mean that is < 3 SDs away from 0. If we do so, we \n","    limit the expected number of negative measurements to 2 per 1000 random\n","    generations - across all of our hormone metrics, this is fewer than 15 \n","    \"completely bogus\" values per 1000 patients which will hopefully hide within\n","    the noise of masking/measurement error. \n","\n","    Finally, for the other metrics not at risk of generating zeros, we want to \n","    sample the means from a range within the bulk of the distribution (e.g. \n","    middle 40%), so there is still some noise in the model that prevents overfitting \n","    due to massive separation of the data (e.g. wildly different mean hormone\n","    metrics by patient response type that could be guessed at by inspection). \n","    \"\"\"\n","    generation_sd = sd/RESAMPLING_CONSTANT\n","    sampling_range_min = max(3*generation_sd, stats.norm.ppf(QUANTILE_WIDTH, mean, sd))\n","    sampling_range_max = mean + (mean-sampling_range_min)\n","    range_boundaries = np.linspace(stats.norm.cdf(sampling_range_min, mean, sd),\\\n","                                   stats.norm.cdf(sampling_range_max, mean, sd),\\\n","                                   hormone_num_ranges+1)\n","    print(range_boundaries)\n","\n","    #Sample the means from each hormone range\n","    sampled_means = []\n","    for i in range(hormone_num_ranges):\n","      sampled_means.append(stats.norm.ppf(np.random.uniform(range_boundaries[i], \\\n","                                                    range_boundaries[i+1]), mean, sd))\n","    #If there are fewer than 10 ranges to be sampled, set the remainder to be \n","    #overall mean \n","    if hormone_num_ranges < 10: \n","      sampled_means = sampled_means + [mean for i in range(10-hormone_num_ranges)]\n","    \n","    #Set dictionary\n","    hormone_ranges[hormone] = sampled_means"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XQHVAm7cfdzM","executionInfo":{"status":"ok","timestamp":1685212769275,"user_tz":420,"elapsed":18,"user":{"displayName":"Bowen Jiang","userId":"16505810748545332074"}},"outputId":"b730a26b-8eca-4203-9541-b779bda454dd"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of ranges for lh_fsh_ratio: 10\n","[0.3  0.34 0.38 0.42 0.46 0.5  0.54 0.58 0.62 0.66 0.7 ]\n","Number of ranges for estradiol: 7\n","[0.3        0.35714286 0.41428571 0.47142857 0.52857143 0.58571429\n"," 0.64285714 0.7       ]\n","Number of ranges for testosterone: 9\n","[0.3        0.34444444 0.38888889 0.43333333 0.47777778 0.52222222\n"," 0.56666667 0.61111111 0.65555556 0.7       ]\n","Number of ranges for progesterone_17oh: 6\n","[0.3        0.36666667 0.43333333 0.5        0.56666667 0.63333333\n"," 0.7       ]\n","Number of ranges for dheas: 6\n","[0.46631374 0.47754249 0.48877125 0.5        0.51122875 0.52245751\n"," 0.53368626]\n","Number of ranges for androstenedione: 8\n","[0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7 ]\n","Number of ranges for amh: 9\n","[0.3        0.34444444 0.38888889 0.43333333 0.47777778 0.52222222\n"," 0.56666667 0.61111111 0.65555556 0.7       ]\n"]}]},{"cell_type":"code","source":["#Examine in a dataframe format\n","response_type_means = pd.DataFrame(hormone_ranges)\n","response_type_means"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"8DEnvxapfd1s","executionInfo":{"status":"ok","timestamp":1685212769275,"user_tz":420,"elapsed":16,"user":{"displayName":"Bowen Jiang","userId":"16505810748545332074"}},"outputId":"fd697eaa-2731-42f2-cc76-9da4ab8db504"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   lh_fsh_ratio  estradiol  testosterone  progesterone_17oh       dheas  \\\n","0      1.404920  78.381501     59.774086           1.148508  692.231438   \n","1      1.427874  81.527586     63.457611           1.325195  732.586451   \n","2      1.516077  85.380144     63.606314           1.539597  755.636403   \n","3      1.604467  88.912061     67.272326           1.626208  788.569311   \n","4      1.679392  89.384372     70.851530           1.935247  817.303654   \n","5      1.742142  92.776554     74.606542           2.119638  863.832703   \n","6      1.836128  98.644721     79.051968           1.600000  777.500000   \n","7      1.891633  87.600000     82.443333           1.600000  777.500000   \n","8      1.915573  87.600000     86.009237           1.600000  777.500000   \n","9      2.022716  87.600000     71.400000           1.600000  777.500000   \n","\n","   androstenedione        amh  \n","0         3.153781  58.182675  \n","1         3.978012  62.942082  \n","2         4.527994  69.142698  \n","3         4.834881  73.196544  \n","4         5.247471  75.973963  \n","5         6.023041  81.461009  \n","6         6.641497  83.597995  \n","7         7.050324  86.445893  \n","8         5.200000  91.698191  \n","9         5.200000  76.000000  "],"text/html":["\n","  <div id=\"df-85011523-3a68-4559-a28e-e61d5379f77c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>lh_fsh_ratio</th>\n","      <th>estradiol</th>\n","      <th>testosterone</th>\n","      <th>progesterone_17oh</th>\n","      <th>dheas</th>\n","      <th>androstenedione</th>\n","      <th>amh</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.404920</td>\n","      <td>78.381501</td>\n","      <td>59.774086</td>\n","      <td>1.148508</td>\n","      <td>692.231438</td>\n","      <td>3.153781</td>\n","      <td>58.182675</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.427874</td>\n","      <td>81.527586</td>\n","      <td>63.457611</td>\n","      <td>1.325195</td>\n","      <td>732.586451</td>\n","      <td>3.978012</td>\n","      <td>62.942082</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.516077</td>\n","      <td>85.380144</td>\n","      <td>63.606314</td>\n","      <td>1.539597</td>\n","      <td>755.636403</td>\n","      <td>4.527994</td>\n","      <td>69.142698</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.604467</td>\n","      <td>88.912061</td>\n","      <td>67.272326</td>\n","      <td>1.626208</td>\n","      <td>788.569311</td>\n","      <td>4.834881</td>\n","      <td>73.196544</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.679392</td>\n","      <td>89.384372</td>\n","      <td>70.851530</td>\n","      <td>1.935247</td>\n","      <td>817.303654</td>\n","      <td>5.247471</td>\n","      <td>75.973963</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1.742142</td>\n","      <td>92.776554</td>\n","      <td>74.606542</td>\n","      <td>2.119638</td>\n","      <td>863.832703</td>\n","      <td>6.023041</td>\n","      <td>81.461009</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1.836128</td>\n","      <td>98.644721</td>\n","      <td>79.051968</td>\n","      <td>1.600000</td>\n","      <td>777.500000</td>\n","      <td>6.641497</td>\n","      <td>83.597995</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1.891633</td>\n","      <td>87.600000</td>\n","      <td>82.443333</td>\n","      <td>1.600000</td>\n","      <td>777.500000</td>\n","      <td>7.050324</td>\n","      <td>86.445893</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1.915573</td>\n","      <td>87.600000</td>\n","      <td>86.009237</td>\n","      <td>1.600000</td>\n","      <td>777.500000</td>\n","      <td>5.200000</td>\n","      <td>91.698191</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>2.022716</td>\n","      <td>87.600000</td>\n","      <td>71.400000</td>\n","      <td>1.600000</td>\n","      <td>777.500000</td>\n","      <td>5.200000</td>\n","      <td>76.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85011523-3a68-4559-a28e-e61d5379f77c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-85011523-3a68-4559-a28e-e61d5379f77c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-85011523-3a68-4559-a28e-e61d5379f77c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["#Looks good, but - we need to shuffle the values in each column independently \n","#to obtain the final patient response type hormone means\n","for column in response_type_means.columns:\n","  means = response_type_means[column].values\n","  np.random.shuffle(means)\n","  response_type_means[column] = means"],"metadata":{"id":"gp5QfeiFfd3q","executionInfo":{"status":"ok","timestamp":1685212769276,"user_tz":420,"elapsed":16,"user":{"displayName":"Bowen Jiang","userId":"16505810748545332074"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["response_type_means"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"Bfaz1YTufd5w","executionInfo":{"status":"ok","timestamp":1685212769276,"user_tz":420,"elapsed":16,"user":{"displayName":"Bowen Jiang","userId":"16505810748545332074"}},"outputId":"f0ef4386-5e1e-49aa-b782-918602391971"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   lh_fsh_ratio  estradiol  testosterone  progesterone_17oh       dheas  \\\n","0      1.836128  78.381501     63.457611           2.119638  777.500000   \n","1      1.516077  81.527586     71.400000           1.600000  777.500000   \n","2      1.427874  92.776554     63.606314           1.935247  755.636403   \n","3      1.404920  87.600000     59.774086           1.600000  788.569311   \n","4      2.022716  88.912061     74.606542           1.325195  692.231438   \n","5      1.742142  89.384372     70.851530           1.600000  817.303654   \n","6      1.915573  87.600000     86.009237           1.600000  777.500000   \n","7      1.679392  85.380144     79.051968           1.148508  732.586451   \n","8      1.604467  87.600000     67.272326           1.626208  863.832703   \n","9      1.891633  98.644721     82.443333           1.539597  777.500000   \n","\n","   androstenedione        amh  \n","0         4.834881  86.445893  \n","1         5.200000  83.597995  \n","2         6.641497  76.000000  \n","3         6.023041  91.698191  \n","4         5.247471  62.942082  \n","5         3.978012  75.973963  \n","6         4.527994  81.461009  \n","7         7.050324  58.182675  \n","8         5.200000  69.142698  \n","9         3.153781  73.196544  "],"text/html":["\n","  <div id=\"df-b0a042bf-4d28-4425-aff7-d4448bf320ed\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>lh_fsh_ratio</th>\n","      <th>estradiol</th>\n","      <th>testosterone</th>\n","      <th>progesterone_17oh</th>\n","      <th>dheas</th>\n","      <th>androstenedione</th>\n","      <th>amh</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.836128</td>\n","      <td>78.381501</td>\n","      <td>63.457611</td>\n","      <td>2.119638</td>\n","      <td>777.500000</td>\n","      <td>4.834881</td>\n","      <td>86.445893</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.516077</td>\n","      <td>81.527586</td>\n","      <td>71.400000</td>\n","      <td>1.600000</td>\n","      <td>777.500000</td>\n","      <td>5.200000</td>\n","      <td>83.597995</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.427874</td>\n","      <td>92.776554</td>\n","      <td>63.606314</td>\n","      <td>1.935247</td>\n","      <td>755.636403</td>\n","      <td>6.641497</td>\n","      <td>76.000000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.404920</td>\n","      <td>87.600000</td>\n","      <td>59.774086</td>\n","      <td>1.600000</td>\n","      <td>788.569311</td>\n","      <td>6.023041</td>\n","      <td>91.698191</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2.022716</td>\n","      <td>88.912061</td>\n","      <td>74.606542</td>\n","      <td>1.325195</td>\n","      <td>692.231438</td>\n","      <td>5.247471</td>\n","      <td>62.942082</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1.742142</td>\n","      <td>89.384372</td>\n","      <td>70.851530</td>\n","      <td>1.600000</td>\n","      <td>817.303654</td>\n","      <td>3.978012</td>\n","      <td>75.973963</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1.915573</td>\n","      <td>87.600000</td>\n","      <td>86.009237</td>\n","      <td>1.600000</td>\n","      <td>777.500000</td>\n","      <td>4.527994</td>\n","      <td>81.461009</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1.679392</td>\n","      <td>85.380144</td>\n","      <td>79.051968</td>\n","      <td>1.148508</td>\n","      <td>732.586451</td>\n","      <td>7.050324</td>\n","      <td>58.182675</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1.604467</td>\n","      <td>87.600000</td>\n","      <td>67.272326</td>\n","      <td>1.626208</td>\n","      <td>863.832703</td>\n","      <td>5.200000</td>\n","      <td>69.142698</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1.891633</td>\n","      <td>98.644721</td>\n","      <td>82.443333</td>\n","      <td>1.539597</td>\n","      <td>777.500000</td>\n","      <td>3.153781</td>\n","      <td>73.196544</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0a042bf-4d28-4425-aff7-d4448bf320ed')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b0a042bf-4d28-4425-aff7-d4448bf320ed button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b0a042bf-4d28-4425-aff7-d4448bf320ed');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["#Load in covariance matrix and prepare: \n","cov = np.load(\"cov_matrix.npy\")\n","cov = cov * (COVARIANCE_STRENGTH*(np.ones((7,7)) - np.identity(7)) + np.identity(7))\n","w, v = np.linalg.eig(cov)\n","sigmas = np.sqrt(w) * v #get covariance-corrected SDs\n","print(f\"Number of negative covariances: {np.sum(np.where(cov < 0, 1, 0))} (of 49 total covariances)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-qhWcHnMgDO1","executionInfo":{"status":"ok","timestamp":1685212769276,"user_tz":420,"elapsed":14,"user":{"displayName":"Bowen Jiang","userId":"16505810748545332074"}},"outputId":"7c188f34-03ee-44c9-ca77-ebb065f9156f"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of negative covariances: 22 (of 49 total covariances)\n"]}]},{"cell_type":"code","source":["#Prepare our calculations for determining symptom prevalence \n","pcos_hormone_means = []\n","pcos_hormone_sds = []\n","for hormone, dist in pcos_hormones.items():\n","    mean, sd = dist\n","    pcos_hormone_means.append(mean)\n","    pcos_hormone_sds.append(sd)\n","pcos_hormone_means = np.array(pcos_hormone_means)\n","pcos_hormone_sds = np.array(pcos_hormone_sds)"],"metadata":{"id":"WdLfSHYqlGOa","executionInfo":{"status":"ok","timestamp":1685212769579,"user_tz":420,"elapsed":315,"user":{"displayName":"Bowen Jiang","userId":"16505810748545332074"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["#Generate lots of patients, one response-type at a time \n","all_patients = {}\n","for i in range(10): \n","  #Indexing column - for response type \n","  response_type = np.repeat(i+1, 10000)[:, np.newaxis]\n","\n","  means = response_type_means.loc[0].values\n","  seeds = np.random.normal(loc=0, scale=(1/RESAMPLING_CONSTANT), size=(means.shape[0], 10000))\n","  #Hormone values \n","  values = (sigmas @ seeds + np.repeat(means[:, np.newaxis], 10000, axis=1)).T\n","\n","  avg_zscores = (values - np.repeat(pcos_hormone_means[np.newaxis, :], 10000, axis=0))/\\\n","  np.repeat(pcos_hormone_sds[np.newaxis, :], 10000, axis=0)\n","  #Avg hormone z-scores (based on overall PCOS means) for calculating symptom freq. \n","  avg_zscores = np.average(avg_zscores, axis=1)\n","\n","  \"\"\"\n","  Symptom Bernoulli probabilities - base distributions as follows (mean +/- SD):\n","  - Irregular menstruation 0.8 +/- 0.5 \n","  - Ovarian cysts 0.75 +/- 0.5\n","  - Hirsutism 0.75 +/- 0.5\n","  - Acne 0.25 +/- 0.1\n","  - Anxiety 0.42 +/- 0.1\n","  - Depression 0.37 +/- 0.1\n","\n","  \"\"\"\n","  symptom_probs = np.concatenate((0.8+0.05*avg_zscores[:,np.newaxis], \n","                                  0.75+0.05*avg_zscores[:,np.newaxis],\n","                                  0.75+0.05*avg_zscores[:,np.newaxis], \n","                                  0.25+0.1*avg_zscores[:,np.newaxis],\n","                                  0.42+0.1*avg_zscores[:,np.newaxis],\n","                                  0.37+0.1*avg_zscores[:,np.newaxis]),\n","                                 axis=1)\n","  symptom_assignments = np.random.uniform(size=symptom_probs.shape)\n","  #Symptom binary variables \n","  symptom_assignments = np.where(symptom_assignments < symptom_probs, 1, 0)\n","\n","  \"\"\"\n","  Successful treatment - treatments will be numbered 1-10 for now \n","  The most frequent treatment for response type 1 will be OCP1; response type 2 -> OCP2, etc.\n","  \"\"\"\n","  treatments = np.random.uniform(size=(10000,1))\n","  treatments = np.where(treatments < PROPORTION_TRUE_TREATMENT, i+1, np.random.randint(1, 11))\n","\n","\n","  columns = ['response_type'] + list(response_type_means.columns) + \\\n","            ['irreg_menstr', 'cysts', 'hirsutism', 'acne', 'anxiety', 'depression', 'treatment']\n","  \n","  #Create data array, remove negative values \n","  data_array = np.concatenate((response_type, values, symptom_assignments, treatments), axis=1)\n","  data_array = np.where(data_array < 0, 0, data_array)\n","\n","  #Create dataframe for all patients of one response type \n","  response_type_temp = pd.DataFrame(data_array)\n","  response_type_temp.columns = columns\n","  all_patients[i+1] = response_type_temp\n"],"metadata":{"id":"r_3VzisaPRw8","executionInfo":{"status":"ok","timestamp":1685212769580,"user_tz":420,"elapsed":5,"user":{"displayName":"Bowen Jiang","userId":"16505810748545332074"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["###Generating 15 clinics - start by getting proportions of patient response types\n","#Use gamma distribution instead of random uniform to generate highly unequal proportions \n","#of patient response types in different clinics\n","clinic_response_type_proportions = np.random.gamma(shape=1.5, scale=1.5, size=(15,10))\n","clinic_response_type_proportions /= np.repeat(np.sum(clinic_response_type_proportions, axis=1)[:, np.newaxis], 10, axis=1)\n","\n","#Now, generate the number of patients in each of 15 clinics using random normal distribution\n","#Assume 1500 +/- 300 patients per practice\n","clinic_sizes = np.repeat(np.round(np.random.normal(loc=1500, scale=300, size=(15,1)), 0), 10, axis=1)\n","\n","#Finally, do element-wise multiplication to find the number of patients per response type to sample per clinic\n","num_responsetypes_per_clinic = np.round(clinic_response_type_proportions * clinic_sizes, 0).astype('int')"],"metadata":{"id":"m9cGERcnrtnM","executionInfo":{"status":"ok","timestamp":1685212769580,"user_tz":420,"elapsed":4,"user":{"displayName":"Bowen Jiang","userId":"16505810748545332074"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["num_responsetypes_per_clinic"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ymZlm1CNRizv","executionInfo":{"status":"ok","timestamp":1685212769580,"user_tz":420,"elapsed":4,"user":{"displayName":"Bowen Jiang","userId":"16505810748545332074"}},"outputId":"3dda1081-a2de-4ce9-c698-b705f1b5ad68"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[100, 479, 349, 173, 154,  97,  73,  28, 235, 154],\n","       [ 80, 195,  29, 152, 128, 203,  88,  39,  29, 303],\n","       [  4,  65,  49, 200,   9,  31,  62, 235,  23, 123],\n","       [ 33, 356, 232,  98,  84,  88,  69,  99, 151,  87],\n","       [ 13, 234, 191,  71,  37,  61,  18,  35,   3, 147],\n","       [128, 313, 230,  73,  82,  64, 122, 352, 338, 221],\n","       [ 73, 160, 157,  83, 248, 397, 212, 171,  68, 380],\n","       [210, 116, 159,  69, 166,  33, 407, 110, 112, 287],\n","       [ 62,   5, 270,  41, 435, 296,  84, 203, 679,  91],\n","       [  1, 165, 226, 202,  78,  52, 139, 183, 289,  41],\n","       [ 67, 590, 142, 132,  98, 143,  48,  45, 172,  26],\n","       [711, 100, 398,  75,  74, 101, 275, 163, 141, 318],\n","       [192,  76, 294, 268,  61, 120,  59, 187,  84, 432],\n","       [124,  51, 175, 304,  49, 166, 174,  47,  19,  60],\n","       [ 38, 297, 119,  37,  37,  71,  20, 193, 120,  41]])"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["#Because the response types are randomly generated, no need to reshuffle the data\n","clinics = {}\n","prev_idxs = np.zeros((10,)).astype('int')\n","for clinic in range(15):\n","  #Slice a row from the array specifying number of each patient response type for the clinic\n","  #This will specify how many more rows of each patient response type dataframe to sample \n","  #and concatenate to form one clinic's worth of data\n","  next_idxs = prev_idxs + num_responsetypes_per_clinic[clinic]\n","\n","  #Fetch rows from the appropriate response type-specific dataframes to make clinic df\n","  clinic_patients = None\n","  for i, response_type in enumerate(next_idxs):\n","    if clinic_patients is None:\n","      clinic_patients = all_patients[i+1].iloc[prev_idxs[i]:next_idxs[i]]\n","    else:\n","      clinic_patients = pd.concat([clinic_patients, all_patients[i].iloc[prev_idxs[i]:next_idxs[i]]], ignore_index=True)\n","  clinics[clinic] = clinic_patients\n","  #Set new index positions for the next round of row slicing\n","  prev_idxs = next_idxs"],"metadata":{"id":"Gz-gT_GpTdct","executionInfo":{"status":"ok","timestamp":1685212769891,"user_tz":420,"elapsed":313,"user":{"displayName":"Bowen Jiang","userId":"16505810748545332074"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["#Add noise per clinic\n","#Use gamma distribution to select \"average percentage hormone noise\" and \n","#\"average symptom masking probability\"\n","\n","clinic_hormone_percent_noise = PER_CLINIC_NOISE_SPREAD*np.random.gamma(shape=1, scale=2, size=(15))\n","clinic_symptom_mask_prob = SYMPTOM_MASK_CONST*np.random.gamma(shape=9, scale=0.5, size=(15))\n","\n","for i in range(15):\n","  print(f\"Clinic {i+1} will have mean 0, SD ({clinic_hormone_percent_noise[i]}% hormone mean) noise added to hormone metrics, and {clinic_symptom_mask_prob[i]}% of symptoms randomly masked\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pc4yzV7oUjKc","executionInfo":{"status":"ok","timestamp":1685212769892,"user_tz":420,"elapsed":3,"user":{"displayName":"Bowen Jiang","userId":"16505810748545332074"}},"outputId":"e8d834b5-73fa-4905-8bc8-84aec71d9433"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Clinic 1 will have mean 0, SD (5.031680942936835% hormone mean) noise added to hormone metrics, and 25.184844067666177% of symptoms randomly masked\n","Clinic 2 will have mean 0, SD (0.8544373604643642% hormone mean) noise added to hormone metrics, and 15.997538036177303% of symptoms randomly masked\n","Clinic 3 will have mean 0, SD (2.2216478098203205% hormone mean) noise added to hormone metrics, and 18.7907921243227% of symptoms randomly masked\n","Clinic 4 will have mean 0, SD (0.18181991598514338% hormone mean) noise added to hormone metrics, and 16.85928281745749% of symptoms randomly masked\n","Clinic 5 will have mean 0, SD (0.5624629619128422% hormone mean) noise added to hormone metrics, and 21.688130995143947% of symptoms randomly masked\n","Clinic 6 will have mean 0, SD (1.5241589793469354% hormone mean) noise added to hormone metrics, and 22.944781943001086% of symptoms randomly masked\n","Clinic 7 will have mean 0, SD (0.36726567451239356% hormone mean) noise added to hormone metrics, and 19.659392493244546% of symptoms randomly masked\n","Clinic 8 will have mean 0, SD (0.0224321907354867% hormone mean) noise added to hormone metrics, and 13.965589183928762% of symptoms randomly masked\n","Clinic 9 will have mean 0, SD (0.1355125113166578% hormone mean) noise added to hormone metrics, and 34.87108546840726% of symptoms randomly masked\n","Clinic 10 will have mean 0, SD (0.4729499709751256% hormone mean) noise added to hormone metrics, and 19.461397078145573% of symptoms randomly masked\n","Clinic 11 will have mean 0, SD (4.043802926013602% hormone mean) noise added to hormone metrics, and 27.390837303752697% of symptoms randomly masked\n","Clinic 12 will have mean 0, SD (1.0152966495058178% hormone mean) noise added to hormone metrics, and 26.97470925441902% of symptoms randomly masked\n","Clinic 13 will have mean 0, SD (4.941932707593249% hormone mean) noise added to hormone metrics, and 22.766952574687682% of symptoms randomly masked\n","Clinic 14 will have mean 0, SD (0.16922919152000734% hormone mean) noise added to hormone metrics, and 19.2084333650402% of symptoms randomly masked\n","Clinic 15 will have mean 0, SD (0.9380963636589577% hormone mean) noise added to hormone metrics, and 37.217176815608894% of symptoms randomly masked\n"]}]},{"cell_type":"code","source":["#Add noise per clinic\n","for clinic, patients in clinics.items():\n","  #For hormone measurements\n","  for hormone in ['lh_fsh_ratio', 'estradiol', 'testosterone', 'progesterone_17oh', 'dheas', 'androstenedione', 'amh']:\n","    measurements = patients[hormone].values \n","    #Create noise vector for each hormone independently, add to hormone measurements \n","    noise = np.random.normal(loc=0, scale=HORMONE_MEASUREMENT_NOISE*0.01*clinic_hormone_percent_noise[clinic]*np.mean(measurements), size=measurements.shape)\n","    patients[hormone] = np.where(measurements + noise < 0, 0, measurements + noise)\n","  #For symptoms\n","  for symptom in ['irreg_menstr', 'cysts', 'hirsutism', 'acne', 'anxiety', 'depression']:\n","    measurements = patients[symptom].values\n","    #Mask out symptoms at the clinic-specified rate \n","    mask_prob = np.random.uniform(size=measurements.shape)\n","    patients[symptom] = np.where(mask_prob < 0.01*clinic_symptom_mask_prob[clinic], 0, measurements).astype('int')\n","  #Set patient response type label (DO NOT USE AS A FEATURE), treatment label as ints \n","  patients['treatment'] = patients.treatment.values.astype('int')\n","  patients['response_type'] = patients.response_type.values.astype('int')\n","  patients.to_csv(f'clinic_datasets/clinic_{clinic}.csv')"],"metadata":{"id":"o2wH3ZttYGTl","executionInfo":{"status":"ok","timestamp":1685212770579,"user_tz":420,"elapsed":689,"user":{"displayName":"Bowen Jiang","userId":"16505810748545332074"}}},"execution_count":16,"outputs":[]}]}